#+LaTeX_CLASS: sendence-com-article-2
#+SETUPFILE: ~/.emacs.dir/org-html-themes/setup/theme-readtheorg.setup
#+TITLE: Performance Tests in AWS
#+AUTHOR: Markus Fix
#+EMAIL: markus@sendence.com
#+DATE: 2016-07-21
#+DESCRIPTION: Notes on how to run performance tests in AWS
#+KEYWORDS: Sendence, distributed, orchestration, buffy, dagon
#+LANGUAGE: english
#+STARTUP: overview
#+TAGS: PROJECT(p) HOME(h) OFFICE(o) PHONE(t) ERRANDS(e)
#+STARTUP: hidestars
#+LaTeX_CLASS_OPTIONS: [10pt,a4paper,captions=tableheading,headsepline,footsepline]
#+LateX_HEADER: \KOMAoptions{titlepage=true, abstract=true}
#+LaTeX_HEADER: \subtitle{Buffy orchestration}
#+LaTeX_HEADER: \usepackage{paralist}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \let\itemize\compactitem
#+LaTeX_HEADER: \let\description\compactdesc
#+LaTeX_HEADER: \let\enumerate\compactenum
#+LaTeX_CLASS_OPTIONS: [captions=tableheading]
#+LATEX:
#+LATEX: \listoffigures

* Introduction
This document describes how to boot and provision a AWS test system
for Buffy performance testing.


* Prepare Dev Box
** Virtualenv on MacBook
Make sure you have =virtualenv= and =virtualenvwrapper= installed on
your MacBook. You can follow along with the [[http://www.marinamele.com/2014/05/install-python-virtualenv-virtualenvwrapper-mavericks.html][installation guide]]. Do the
following in a shell on your MacBook:
#+BEGIN_SRC sh
brew update
brew upgrade
brew install python
pip install virtualenv
pip install virtualenvwrapper
mkdir ~/.virtualenvs
#+END_SRC

Add this to your =.bashrc= file:
#+BEGIN_SRC sh
export WORKON_HOME=~/.virtualenvs
source /usr/local/bin/virtualenvwrapper.sh
#+END_SRC

** Install JQ
#+BEGIN_SRC sh
brew install jq
#+END_SRC

** Install Terraform
#+BEGIN_SRC sh
brew install terraform
#+END_SRC

** Create Ansible Vault
Get the vault password from your friendly sysadmin and echo it into
the vault password file.
#+BEGIN_SRC sh
echo "<XXXXXXX>" > ~/.ansible_vault_pass.txt
#+END_SRC

** Install Ansible and Boto
#+BEGIN_SRC sh
pip install --upgrade setuptools --user python
pip uninstall ansible
pip uninstall boto
pip install -Iv ansible==2.0.0.2
pip install -Iv boto==2.39.0
#+END_SRC

** Boto AWS Credentials
Create a file =$HOME/.boto= with this contents (using your AWS access
keys in place of the =xxx=):
#+BEGIN_SRC sh
[Credentials]
aws_access_key_id = xxx
aws_secret_access_key = xxx
#+END_SRC

** AWS Credentials
Create a directory for AWS credentials.
#+BEGIN_SRC sh
mkdir $HOME/.aws
#+END_SRC

Login to the AWS web console: https://sendence.signin.aws.amazon.com/console
1. Select "Identity and Access Management" from the list of AWS services.
2. Select "Users" form the menu.
3. Search for your username and select it.
4. Click on "Create Access Key".
5. Click on "Download Credentials".
6. Open the downloaded file named =credentials.csv= in a text editor.

The file you're editing is formatted like this:
#+BEGIN_EXAMPLE
User Name,Access Key Id,Secret Access Key
"markus",AKIAI4W7TVYRVTMQXXXX,pFc6lBX4bSx7HFHG736Ur1B1oRpRCL82xphBXXXX
#+END_EXAMPLE

Edit it too look like this:
#+BEGIN_EXAMPLE
[default]
aws_access_key_id = AKIAI4W7TVYRVTMQXXXX
aws_secret_access_key = pFc6lBX4bSx7HFHG736Ur1B1oRpRCL82xphBXXXX
#+END_EXAMPLE

Save the file into the directory =$HOME/.aws= and name it =credentials=.

** AWS CLI Tools
Install the AWS command-line tools:
#+BEGIN_SRC sh
brew install awscli
#+END_SRC

Test your AWS credentials:
#+BEGIN_SRC sh
aws ec2 describe-instances
#+END_SRC

** AWS SSH Certificate
To login to EC2 instances created by the makefile you need to install the SSH certificates for each region. The example below shows how to do that for the use-east-1 region.
#+BEGIN_SRC sh
mkdir $HOME/.ssh/ec2
cp us-east-1.pem $HOME/.ssh/ec2/us-east-1.pem
#+END_SRC
Ask you friendly sysadmin to provide those =.pem= files.

** Github Personal Access Token
*Please make sure to give the token only minimal privileges!*

(tbd: explain what the above means)

We need to create a Personal Access token for Github. We will use the
token when checking out the Buffy sources from Github.
Open [[https://github.com/settings/tokens][https://github.com/settings/tokens]] and create a token named
=Buffy-AWS-perf-test-token=. Make sure you copy the token string and
save it in a secure place.

Create a new file named =~/.buffy-aws-perf-test-token= with the
following lines:
#+BEGIN_EXAMPLE
[user]
        name = <your name>
        email = <email address used on Github>
[github]
        user = <your github username>
        token = <your token>
#+END_EXAMPLE

We will copy this file to your test box later in this guide.

* Prepare Test Node
Checkout the buffy repository locally (on your dev box).
#+BEGIN_SRC sh
git clone https://github.com/Sendence/buffy.git
#+END_SRC

Boot and provision a single c4.4xlarge EC2 node using spot pricing.
#+BEGIN_SRC sh
cd buffy/orchestration/terraform
make cluster num_followers=0 leader_instance_type=c4.4xlarge
#+END_SRC

Result:
#+BEGIN_SRC sh
PLAY RECAP *********************************************************************
54.209.159.121             : ok=44   changed=23   unreachable=0  failed=0
#+END_SRC

If you want to later destroy the node/cluster just run this command in
the =buffy/orchestration/terraform= directory:
#+BEGIN_SRC sh
make destroy
#+END_SRC

If you run into trouble during the provisioning phase (or see any
errors after logging into the test box) you can try to run the
provisioner again:
#+BEGIN_SRC sh
make configure
#+END_SRC

Copy the Github credentials to the test node:
#+BEGIN_SRC sh
scp -i ~/.ssh/ec2/us-east-1.pem \
 ~/.buffy-aws-perf-test-token \
 ubuntu@54.209.159.121:~/.gitconfig
we #+END_SRC

Login to the node:
#+BEGIN_SRC sh
ssh -i ~/.ssh/ec2/us-east-1.pem ubuntu@54.209.159.121
#+END_SRC

Clone Buffy:
#+BEGIN_SRC sh
git clone https://github.com/Sendence/buffy.git buffy
#+END_SRC

We are using the pre-build ponyc container to build the binaries:
#+BEGIN_SRC sh
cd buffy
make arch=amd64
#+END_SRC

Build the container images if you intend to use them later:
#+BEGIN_SRC sh
make arch=amd64 build-docker
#+END_SRC

This will build the container images and publish them to the local Docker registry.
If you want to publish the resulting images to the Sendence registry you will need to run he publish command:
#+BEGIN_SRC sh
make arch=amd64 push-docker
#+END_SRC

* Run Performance Tests
** Run as Processes
We will use the simple =double-divide.ini= configuration to demonstrate the boot process.
#+BEGIN_SRC sh
cd dagon
./dagon -d -t 10 --filepath=double-divide.ini --phone-home=127.0.0.1:8080
#+END_SRC


** Run as Containers
*** Preparations
While logged in on the test node get the IP address of the main ethernet interface:
#+BEGIN_SRC sh
ifconfig |grep -A 2 ens3
#+END_SRC

Result:
#+BEGIN_EXAMPLE
ens3      Link encap:Ethernet  HWaddr 06:61:0e:2f:98:b1
          inet addr:10.0.28.248  Bcast:10.0.31.255  Mask:255.255.224.0
          inet6 addr: fe80::461:eff:fe2f:98b1/64 Scope:Link
#+END_EXAMPLE

Our internal (VPC) IP address for the test node is =10.0.28.248=.

The ethernet interface names depend on the EC2 instance type. In case the command above does not yield an IP address use this:
#+BEGIN_SRC sh
ifconfig |grep -A 2 eth
#+END_SRC

Copy the =docker-double-divide.ini= configuration file to =/tmp= before starting the test. Copy the =count-to-hundred.txt= data file to =/tmp= if you want to read the numbers from a file (optional):
#+BEGIN_SRC sh
cp dagon/docker-double-divide-aws-leader.ini /tmp
cp dagon/count-to-hundred.txt /tmp #optional
#+END_SRC



*** Boot Metrics-UI (optional)
Pull the Docker image for the UI from the Sendence registry:
#+BEGIN_SRC sh
docker pull docker.sendence.com:5043/buffy-metrics-ui
#+END_SRC

Boot the metrics UI:
#+BEGIN_SRC sh
docker run -d -u 1000 \
 -p 0.0.0.0:4000:4000 \
 --name mui -h mui --net=buffy-leader \
 docker.sendence.com:5043/buffy-metrics-ui
#+END_SRC

*** Boot Metrics-Receiver
Start the metrics receiver:
 #+BEGIN_SRC sh
docker run -d -u 1000 \
 --name metrics  -h metrics  \
 --privileged \
 -v /usr/bin:/usr/bin:ro  -v /var/run/docker.sock:/var/run/docker.sock  \
 -v /bin:/bin:ro  -v /lib:/lib:ro  -v /lib64:/lib64:ro  \
 -v /usr:/usr:ro  -v /tmp:/tmp  -w /tmp  \
 --net=buffy-leader \
 docker.sendence.com:5043/sendence/double-divide.amd64:<docker-image-tag> \
 --run-sink -r -l metrics:9000 -m mui:5001 \
 --name double-divide --period 1 -a double-divide-app
 #+END_SRC

*** Boot Dagon
Boot the Dagon container with the =docker-double-divide.ini= config file that defines the topology. Make sure you replace the =<internal IP address>= placeholder with the internal VPC address of the test node. Replace the =<docker-image-tag>= placeholder with the Docker image tag of your build run.
#+BEGIN_SRC sh
docker run -u 0 \
 --name dagon  -h dagon  --privileged  -i  \
 -e LC_ALL=C.UTF-8 -e LANG=C.UTF-8 \
 -e "constraint:node==ip-10-0-28-248" \
 -v /usr/bin:/usr/bin:ro  \
 -v /var/run/docker.sock:/var/run/docker.sock  \
 -v /bin:/bin:ro  -v /lib:/lib:ro  -v /lib64:/lib64:ro \
 -v /usr:/usr:ro  -v /tmp:/tmp  -w /tmp  \
 --net=buffy-leader \
 docker.sendence.com:5043/sendence/dagon.amd64:<docker-image-tag> \
 dagon.amd64 \
 --docker=tcp://<internal IP address>:2375  \
 -t 30  \
 --filepath=/tmp/docker-double-divide-aws-leader.ini  \
 --phone-home=dagon:8080 \
 --tag=<docker-image-tag>
#+END_SRC

* Appendix
** Ponyc Compiler Install


** Edit Remote Files
*** Howto edit files remotely
*** Howto mount a remote file system via SSH
*** Emacs Tramp

** Docker Swarm Cluster
Performance tests using containers on a Docker Swarm cluster on top of EC2 nodes need to optimize the following parameters:
- Use placement groups
- Use high network bandwidth instance types
* Dipin Tuning
NOTE: It is possible to reconfigure a cluster by using the appropriate
make command with =configure= instead of =cluster=.

**  ISOCPUS Off
Make cluster of 1 leader node only with 2 cpus reserved for system use
and the rest for user processes and the =isolcpus= boot option
disabled.

*** (create)
#+BEGIN_SRC sh
make cluster cluster_name=dh num_followers=0 \
 force_instance=c3.4xlarge ansible_num_system_cpus=2
#+END_SRC

*** (configure)
#+BEGIN_SRC sh
make configure cluster_name=dh num_followers=0 \
 force_instance=c3.4xlarge ansible_num_system_cpus=2
#+END_SRC

** ISOCPUS On
Make cluster of 1 leader node only with 2 cpus reserved for
system use and the rest for user processes and the =isolcpus= boot
option enabled.

*** (create)
#+BEGIN_SRC sh
make cluster cluster_name=dh num_followers=0 \
force_instance=c3.4xlarge ansible_num_system_cpus=2 ansible_isolcpus=true
#+END_SRC
Result:
#+BEGIN_EXAMPLE
PLAY RECAP *********************************************************************
54.165.9.39                : ok=70   changed=39   unreachable=0    failed=0
#+END_EXAMPLE

*** (configure)
#+BEGIN_SRC sh
make configure cluster_name=dh num_followers=0 \
force_instance=c3.4xlarge ansible_num_system_cpus=2 ansible_isolcpus=true
#+END_SRC

** Run stress-ng to burn cpu on all cores within default/system cpuset
#+BEGIN_SRC sh
ssh -i ~/.ssh/ec2/us-east-1.pem ubuntu@54.165.9.39
sudo apt-get install -y stress-ng
stress-ng --cpu 0 --cpu-method all -t 1m
#+END_SRC

*** Confirm CPU is Pinned
In another shell, run htop to confirm cpu is pinned into system
isolated cpus.

#+BEGIN_SRC sh
ssh -i ~/.ssh/ec2/us-east-1.pem ubuntu@54.165.9.39
htop
#+END_SRC

** Run stress-ng to burn cpu on all cores within user cpuset
Must use sudo else command is executed in default/system cpuset
silently.

#+BEGIN_SRC sh
sudo cset proc -s user -e stress-ng -- --cpu 0 --cpu-method all -t 1m
#+END_SRC

*** Confirm CPU is Pinned
In another shell, run =htop= to confirm cpu is pinned into user isolated
cpus.

** NUMACTL
Run =stress-ng= to burn cpu within user cpuset bound to specific
cpu nodes via =numactl=.

Note: Must use sudo else command is executed in default/system cpuset silently.

#+BEGIN_SRC sh
sudo cset proc -s user -e numactl -- -C 4-6 stress-ng --cpu 14 \
 --cpu-method all -t 1m
#+END_SRC

*** Confirm CPU is Pinned
In another shell, run htop to confirm cpu is pinned into user isolated
cpus.
#+BEGIN_SRC sh
htop
#+END_SRC

** Taskset
Run stress-ng to burn cpu within user cpuset bound to specific cpu
nodes via =taskset=.

Note: Must use sudo else command is executed in default/system cpuset
silently.

#+BEGIN_SRC sh
sudo cset proc -s user -e taskset -- -c 4-6 stress-ng --cpu 14 \
 --cpu-method all -t 1m
#+END_SRC

*** Confirm CPU is Pinned
In another shell, run htop to confirm cpu is pinned into user isolated
cpus.
#+BEGIN_SRC sh
htop
#+END_SRC

** Taskset with =chrt SCHED_FIFO=
Run =stress-ng= to burn cpu within user cpuset bound to specific cpu
nodes via =taskset= using =chrt= to run with =SCHED_FIFO= scheduling.

Note: Must use sudo else command is executed in default/system cpuset
silently.

#+BEGIN_SRC sh
sudo cset proc -s user -e taskset -- -c 4-6 chrt -f 80 \
 stress-ng --cpu 14 --cpu-method all -t 1m
#+END_SRC

*** Confirm CPU is Pinned
In another shell, run htop to confirm cpu is pinned into user isolated
cpus.
#+BEGIN_SRC sh
htop
#+END_SRC

*** Check Scheduling Priority
Run the command below to check scheduling priority of stress-ng.
#+BEGIN_SRC sh
ps aux | grep -i stress-ng | grep -v grep | grep -v sudo | \
 grep -v docker | awk '{print $2}' | xargs -L 1 chrt -p
#+END_SRC

** Run stress-ng in docker to burn cpu on all cores
#+BEGIN_SRC sh
docker run -it --privileged  -v /usr/bin:/usr/bin:ro  \
 -v /var/run/docker.sock:/var/run/docker.sock  -v /bin:/bin:ro \
 -v /lib:/lib:ro  -v /lib64:/lib64:ro  -v /usr:/usr:ro \
 --entrypoint stress-ng \
 docker.sendence.com:5043/sendence/dagon.amd64:sendence-2.3.0-683-g6680f9e \
 --cpu 0 --cpu-method all -t 1m
#+END_SRC

*** Check CPU is Pinned
In another shell, run htop to confirm cpu is pinned into all cpus.
#+BEGIN_SRC sh
htop
#+END_SRC

** Docker CPUset Feature
Run stress-ng in docker to burn cpu bound to specific cpu nodes using
docker cpuset feature.
#+BEGIN_SRC sh
docker run -it --privileged  -v /usr/bin:/usr/bin:ro  \
 -v /var/run/docker.sock:/var/run/docker.sock  -v /bin:/bin:ro  \
 -v /lib:/lib:ro  -v /lib64:/lib64:ro  -v /usr:/usr:ro \
 --entrypoint stress-ng --cpuset-cpus 4-6 \
 docker.sendence.com:5043/sendence/dagon.amd64:sendence-2.3.0-683-g6680f9e \
 --cpu 0 --cpu-method all -t 1m
#+END_SRC

** Docker CPUset and Chrt SCHED_FIFO
Run stress-ng in docker to burn cpu bound to specific cpu nodes using
docker cpuset feature using chrt to run with SCHED_FIFO scheduling.

Note: Must use sudo else command is executed in default/system cpuset
silently.
#+BEGIN_SRC sh
docker run -it --privileged  -v /usr/bin:/usr/bin:ro  \
 -v /var/run/docker.sock:/var/run/docker.sock  -v /bin:/bin:ro  \
 -v /lib:/lib:ro  -v /lib64:/lib64:ro  -v /usr:/usr:ro \
 --entrypoint chrt --cpuset-cpus 4-6 \
 docker.sendence.com:5043/sendence/dagon.amd64:sendence-2.3.0-683-g6680f9e \
 -f 80 stress-ng --cpu 14 --cpu-method all -t 1m
#+END_SRC


*** Confirm CPU is Pinned
In another shell, run htop to confirm cpu is pinned into user isolated
cpus and run the command below to check scheduling priority of =stress-ng=.
#+BEGIN_SRC sh
ps aux | grep -i stress-ng | grep -v grep | grep -v sudo | \
 grep -v docker | awk '{print $2}' | xargs -L 1 chrt -p
#+END_SRC


*** NOTE
When using =ansible_isolcpus=true= as a make argument the
workload for =stress-ng= is not spread across cpus automatigally anymore
because the linux kernel doesn’t manage thread load balancing for
processes running on these cores any longer.
This is exactly what we want because pony will take care of
managing/spreading it’s own threads.

** Running Double-Divide with Tuning Enabled
Giles receiver:
#+BEGIN_SRC sh
sudo cset proc -s user -e numactl -- -C 2-5 chrt -f 80 \
 giles/receiver/receiver -l 127.0.0.1:8000 --ponythreads 1 \
 -e 1000000 -m -w
#+END_SRC

Buffy:
#+BEGIN_SRC sh
sudo cset proc -s user -e numactl -- -C 6-12 chrt -f 80 \
 apps/double-divide/double-divide -l -w 0 -c 127.0.0.1:6000 \
 -d 127.0.0.1:6001 -r 127.0.0.1:7000 -k 127.0.0.1:8000 \
 -n leader -p 127.0.0.1:11000 -m 127.0.0.1:9000 --ponythreads 3
#+END_SRC

Giles sender:
#+BEGIN_SRC sh
sudo cset proc -s user -e numactl -- -C 13-15 chrt -f 80 \
 giles/sender/sender -h 127.0.0.1:7000 -m 1000000 \
 --ponythreads 1 -s 500
#+END_SRC

** Running Market-Spread with Tuning Enabled
Giles receiver:
#+BEGIN_SRC sh
sudo cset proc -s user -e numactl -- -C 2-4 chrt -f 80 \
 giles/receiver/receiver -l 127.0.0.1:8000 --ponythreads 1 \
 -e 1000000 -m -w
#+END_SRC

Buffy:
#+BEGIN_SRC sh
sudo cset proc -s user -e numactl -- -C 5-11 chrt -f 80 \
 apps/market-spread/market-spread -l -w 0 -c 127.0.0.1:6000 \
 -d 127.0.0.1:6001 -r 127.0.0.1:7000,127.0.0.1:7001 \
 -k 127.0.0.1:8000,127.0.0.1:8001,127.0.0.1:8002 -n leader \
 -p 127.0.0.1:11000 -m 127.0.0.1:9000 --ponythreads 3
#+END_SRC

Giles sender:
#+BEGIN_SRC sh

sudo cset proc -s user -e numactl -- -C 12-13 chrt -f 80 \
 giles/sender/sender -h 127.0.0.1:7000 -m 2926 -s 300 \
 -i 2_500_000 -f testing/data/market_spread/nbbo/350-symbols_initial-nbbo-fixish.msg \
 -r --ponythreads=1 -y -g 46 -w

sudo cset proc -s user -e numactl -- -C 12-13 chrt -f 80 \
 giles/sender/sender -h 127.0.0.1:7000 -m 5000000 \
 -s 300 -i 5_000_000 -f testing/data/market_spread/orders/350-symbols_orders-fixish.msg \
 -r --ponythreads=1 -y -g 57 -w

sudo cset proc -s user -e numactl -- -C 14-15 chrt -f 80 \
 giles/sender/sender -h 127.0.0.1:7001 -m 10000000 \
 -s 300 -i 2_500_000 -f testing/data/market_spread/nbbo/350-symbols_nbbo-fixish.msg \
 -r --ponythreads=1 -y -g 46 -w
#+END_SRC



** Documentation Links
*** Links
1. https://github.com/lpechacek/cpuset/blob/master/doc/tutorial.txt
2. http://stackoverflow.com/questions/9392415/linux-sched-other-sched-fifo-and-sched-rr-differences
3. http://linux.die.net/man/1/chrt
4. http://linux.die.net/man/1/taskset
5. http://linux.die.net/man/8/numactl

*** Script for Kernel Tweaks
(sysctl stuff, etc)
#+BEGIN_SRC sh
<buffy repo>/orchestration/ansible/playbooks/roles/common/files/kerneltweaks.sh
#+END_SRC

*** Script That Creates the Cpusets to Isolate Cpu Cores
#+BEGIN_SRC sh
<buffy repo>/orchestration/ansible/playbooks/roles/common/files/create_cpu_shield.sh
#+END_SRC
